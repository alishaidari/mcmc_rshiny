---
title: 'CMDA/STAT 4664: <br> Final Project - Interactive MCMC Rshiny Application'
author: "Ali Haidari"
date: "Spring 2023"
output: html_document
keep_tex: yes
latex_engine: xelatex
html_document: default
graphics: yes
#runtime: shiny
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Documents/CMDA 4664 - Computationally Intensive Stochastic Modeling/")
set.seed(4444)
library(shiny)
library(FAmle)
library(metropolis)
library(mcmc)
library(shinythemes)
```
--- --- ---

## Understanding the Metropolis Algorithm
What is the Metropolis-Hastings Algorithm? This algorithm is a form of Markov Chain Monte Carlo (MCMC) simulation. At a high level overview, the method approximates a posterior distribution based off of a prior fit or prior distribution. We can delve into how the true mechanics of this black box work and how it can be useful for applications within data science.

- The algorithm starts with an initial value of a parameter that we want to estimate. At each step, the algorithm proposes a new value for the parameter by adding a small amount of random noise to the current value. The amount of noise added is called the step size or the proposal distribution.

- The proposed value is then accepted or rejected based on a acceptance ratio, which is the ratio of the target probability density function (PDF).

- The algorithm repeats this process for a specified number of steps, generating a sequence of parameter values. The sequence of values converges to the target distribution as the number of steps increases.

--- --- ----

## Mathmatical Notation of Algorithm
Start with an initial value of the parameter of interest, $\theta_0$.
For each iteration $i = 1, 2, \ldots, N$:

a. Generate a proposal value $\theta_i^*$ from a proposal distribution $q(\cdot|\theta_{i-1})$. 

b. Compute the acceptance probability:
$r \gets \frac{f(\theta_{i}^{})}{f(\theta_{i-1})} \frac{q(\theta_{i-1}|\theta_{i}^{})}{q(\theta_{i}^{}|\theta_{i-1})}$

c. Compare $r$ to a uniform random number $u \sim Uniform(0,1)$:

If $u < r$, accept the proposal and set $\theta_i = \theta_i^*$.
Otherwise, reject the proposal and set $\theta_i = \theta_{i-1}$.
After $N$ iterations, the resulting sequence of parameter values ${\theta_1, \theta_2, \ldots, \theta_N}$ is a sample from the target distribution $f(\theta)$.


--- --- --- 

## Usecases and Motivation

- Bayesian inference: In Bayesian inference, we often need to compute the posterior distribution of a parameter given some data. However, in many cases the posterior distribution is complex and cannot be evaluated directly. The Metropolis-Hastings algorithm provides a way to simulate samples from the posterior distribution, which can then be used to estimate summary statistics or make predictions.

- Image and signal processing: The Metropolis-Hastings algorithm can be used to estimate parameters of image or signal models that are difficult to compute exactly. For example, the algorithm can be used to estimate the parameters of a Bayesian image denoising model, where the posterior distribution is complex and cannot be evaluated directly.

- Optimization: The Metropolis-Hastings algorithm can also be used as a heuristic for finding the maximum or minimum of a complex objective function. By using a proposal distribution that samples from the region around the current solution, the algorithm can explore the parameter space and converge to a global maximum or minimum.

## Limitations
- Slow convergence: The algorithm can be slow to converge to the target distribution, especially if the proposal distribution is not well-tuned to the target distribution. This can result in long simulation times and large numbers of iterations.

- Difficulty in choosing proposal distribution: Choosing an appropriate proposal distribution can be challenging, especially if the target distribution is high-dimensional or has complex structure. A poorly chosen proposal distribution can result in slow convergence or poor mixing of the Markov chain.

- High autocorrelation: The samples generated by the Metropolis-Hastings algorithm can have high autocorrelation, which means that successive samples are highly correlated with each other. This can lead to problems with computing summary statistics, such as standard errors or confidence intervals.


## Running the Interactive MCMC Visualizations
```{r echo=FALSE}

#uncomment when ready to run interactive app
#runApp("mcmc_app.R")

print("Go back and uncomment 'runApp(mcmc_app.R)' to run interactive demo!")

```
